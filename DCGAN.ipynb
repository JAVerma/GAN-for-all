{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "spatial-authorization",
   "metadata": {},
   "source": [
    "# DC-GAN (Unsupervised) from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-criticism",
   "metadata": {},
   "source": [
    "## GAN provide an attractive alternative to maximum likelihood techniques with lack of a heuristic cost function. But Gan are unstable to train, thus producing non sensical outputs.\n",
    "One of the main constraint of neural networks is that they are kind of black box models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-wellington",
   "metadata": {},
   "source": [
    "## In order to develop DC-GAN\n",
    "1. All convolutional net which replaces determinstic spatial pooling functions with strided convolutions, allowing the network to learn its own spatial downsampling. This approach is used to make generator, thus allows to learn spatial upsampling, and discriminator.\n",
    "2. Fully connected layers are eliminated. \n",
    "3. Batch normalization which stabalises learning by normalizing the input to each unit to have zero mean and unit varaiance. This helps deal with training problems that arise due to poor initialization and helps gradient flow in deeper models. This prevents from collapsins all samples to  single point. Also, this tackle the training problems associated with poor initialization and helps gradient flow in deeper models.\n",
    "4. Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n",
    "5. Use LeakyReLU activation in the discriminator for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tough-argentina",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efe353693f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-delta",
   "metadata": {},
   "source": [
    "tensor +1/2 is done in order to shift to lighter color for better visualization \n",
    "Function for visualizing images: \n",
    "Given a tensor of images, \n",
    "number of images, and\n",
    "size per image\n",
    "plots and prints the images in an uniform grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liked-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(tensor, RowsXCols=25, size=(1, 28, 28)):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    tensor = (tensor + 1) / 2\n",
    "    unflat = tensor.detach().cpu()\n",
    "    grid = make_grid(unflat[:RowsXCols], nrow=5)\n",
    "    plt.imshow(grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-terry",
   "metadata": {},
   "source": [
    "# Building discriminator\n",
    " Discriminator Class\n",
    " Values:\n",
    "        im_chan: the number of channels of the output image, a scalar\n",
    "              (MNIST is black-and-white, so 1 channel is your default)\n",
    "    hidden_dim: the inner dimension, a scalar\n",
    "    input 64*64, 1 channel\n",
    " output = (input+2*padding-2*size of kernel)/stride +1\n",
    " so at 1 layer- 64+2-4/2 +1 =32\n",
    " at 2-16\n",
    " at 3- 4\n",
    " at 4-1 \n",
    " \n",
    "  forward function-  for completing a forward pass of the discriminator: Given an image tensor, \n",
    "    returns a 1-dimension tensor representing fake/real.\n",
    "    Parameters:\n",
    "        image: a flattened image tensor with dimension (im_dim)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forbidden-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, im_chan=1, hidden_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv2d(im_chan, hidden_dim,kernel_size=4,stride=2,padding=1), \n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self.disc_block(hidden_dim, hidden_dim * 2,4,2,1), \n",
    "            self.disc_block(hidden_dim*2, hidden_dim * 4,4,2,1),\n",
    "            self.disc_block(hidden_dim*4, hidden_dim * 8,4,2,1),\n",
    "            nn.Conv2d(hidden_dim * 8, 1,kernel_size=4, stride=2,padding=0), \n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "         \n",
    "    def disc_block(self, input_channels, output_channels, kernel_size, stride,padding):\n",
    "      \n",
    "      return nn.Sequential(nn.Conv2d(input_channels, output_channels, kernel_size, stride, padding),\n",
    "                           nn.BatchNorm2d(output_channels),\n",
    "                           nn.LeakyReLU(0.2, inplace=True))\n",
    "      \n",
    "\n",
    "    \n",
    "    def forward(self, image):\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-abortion",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "lasting-lesson",
   "metadata": {},
   "source": [
    "self.forward() is similar to call method but with registered hooks. This is used to directly call a method in the class when an instance name is called. These methods are inherited from nn.Module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-laundry",
   "metadata": {},
   "source": [
    "# making Generator\n",
    "Generator will take a random noise i.e. 100 dimensionalvector which will be upscaled to 1024 channels and then four by four.\n",
    "After that it upscales to 8*8 with 512 channels then it keeps on going to achieve 64*64 with RGB channel.\n",
    "final feature we want 64 thus to convert it to 1024 at 1 upscaling =16 X 64\n",
    "output=(input-1)X stride + outputpadding - 2Xpaddding + kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wanted-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=100, im_chan=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            self.gen_block(z_dim, hidden_dim * 16,4,2,0), #image=4*4\n",
    "            self.gen_block(hidden_dim * 16, hidden_dim * 8, 4,2,1), #image=8*8\n",
    "            self.gen_block(hidden_dim *8, hidden_dim*4, 4, 2 ,1), #image=16*16\n",
    "            self.gen_block(hidden_dim *4, hidden_dim*2, 4, 2 ,1), #image=32*32\n",
    "            self.gen_block(hidden_dim*2, im_chan, kernel_size=4, stride=2,padding=1,final_layer=True),  #64*64\n",
    "        )\n",
    "\n",
    "    def gen_block(self, input_channels, output_channels, kernel_size, stride, padding, final_layer=False):\n",
    "        \n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride,padding),\n",
    "                 nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else: # Final Layer\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride,padding),\n",
    "                nn.Tanh(),\n",
    "                \n",
    "            )\n",
    "\n",
    "    def unsqueeze_noise(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns a copy of that noise with width and height = 1 and channels = z_dim.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, z_dim)\n",
    "        '''\n",
    "        return noise.view(len(noise), self.z_dim, 1, 1)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = self.unsqueeze_noise(noise)\n",
    "        return self.gen(x)\n",
    "\n",
    "def get_noise(n_samples, z_dim, device='cpu'):\n",
    "    \n",
    "    return torch.randn(n_samples, z_dim, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-stranger",
   "metadata": {},
   "source": [
    "# Next thing is weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "least-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "display_step = 500\n",
    "z_dim=100\n",
    "batch_size = 128\n",
    "# A learning rate of 0.0002 works well on DCGAN\n",
    "lr = 0.0002\n",
    "\n",
    "beta_1 = 0.5 \n",
    "beta_2 = 0.999\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    MNIST(root=\"dataset/\", download=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "gen = Generator(z_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "disc = Discriminator().to(device) \n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)\n",
    "n_epochs = 20\n",
    "cur_step = 0\n",
    "mean_generator_loss = 0\n",
    "mean_discriminator_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "provincial-configuration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051c86000979414685e404278db44f0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17197/3543144896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mdisc_fake_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mgen_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_fake_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_fake_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mgen_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mgen_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches\n",
    "    for real, _ in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        real = real.to(device)\n",
    "\n",
    "        ## Update discriminator ##\n",
    "        disc_opt.zero_grad()\n",
    "        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "        fake = gen(fake_noise)\n",
    "        disc_fake_pred = disc(fake.detach())\n",
    "        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "        disc_real_pred = disc(real)\n",
    "        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "\n",
    "        # Keep track of the average discriminator loss\n",
    "        mean_discriminator_loss += disc_loss.item() / display_step\n",
    "        # Update gradients\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        # Update optimizer\n",
    "        disc_opt.step()\n",
    "\n",
    "        ## Update generator ##\n",
    "        gen_opt.zero_grad()\n",
    "        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)\n",
    "        fake_2 = gen(fake_noise_2)\n",
    "        disc_fake_pred = disc(fake_2)\n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        # Keep track of the average generator loss\n",
    "        mean_generator_loss += gen_loss.item() / display_step\n",
    "\n",
    "        ## Visualization code ##\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            print(f\"Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}, epoch :{epoch}\")\n",
    "            showimage(fake)\n",
    "            showimage(real)\n",
    "            mean_generator_loss = 0\n",
    "            mean_discriminator_loss = 0\n",
    "        cur_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-nevada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-business",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-mongolia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-moment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
